### Purpose

- Create Azure Data Factory service with Terraform // Done
- Create a linked service to Data Factory // Continue from this!
  - Was tricky with terraform, try in UI
- Linked service should be Databricks
- Create a simple notebook to Databricks
- Orchestrate it from Data Factory

### How to link ADF and Azure Databricks
1. Have existing resource group / done 
2. Have a data factory service / done 
3. Have a databricks workspace  / done 
4. Use existing cluster or create new
5. Link databricks and data factory from linked service



### Data factory publishing
- When data factory is not connected to any repo, it said to operate in Live mode
- In this mode, publishing means saving


### ADF Pipeline vs Data Flow
- Pipelines are mainly primarily used for orchestration and scheduling
- Orchestrate different set of actions
- ADF data flows are designed for data transformation
- They use spark clusters managed by ADF 
- Data flow definitions are done in the UI, enabling low/no-code approach


### ADF Data flow
- Data transformations without code -> resulting data flows are executed as activities within Azure Data Factory Pipelines
- 